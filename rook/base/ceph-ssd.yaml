apiVersion: v1
kind: Namespace
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "-1"
  labels:
    pod-security.cybozu.com/policy: privileged
    team: csa
  name: ceph-ssd
---
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "-1"
    resize.topolvm.io/enabled: "true"
  name: ceph-ssd-block
parameters:
  clusterID: ceph-ssd
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: ceph-ssd
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/node-stage-secret-namespace: ceph-ssd
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: ceph-ssd
  imageFeatures: layering
  imageFormat: "2"
  pool: ceph-ssd-block-pool
provisioner: ceph-ssd.rbd.csi.ceph.com
reclaimPolicy: Delete
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/created-by: helm
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/part-of: rook-ceph-operator
    helm.sh/chart: rook-ceph-v1.8.3
    operator: rook
    storage-backend: ceph
  name: rook-ceph-cmd-reporter
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/created-by: helm
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/part-of: rook-ceph-operator
    helm.sh/chart: rook-ceph-v1.8.3
    operator: rook
    storage-backend: ceph
  name: rook-ceph-mgr
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/created-by: helm
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/part-of: rook-ceph-operator
    helm.sh/chart: rook-ceph-v1.8.3
    operator: rook
    storage-backend: ceph
  name: rook-ceph-osd
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-ceph-purge-osd
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    app.kubernetes.io/created-by: helm
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/part-of: rook-ceph-operator
    helm.sh/chart: rook-ceph-v1.8.3
    operator: rook
    storage-backend: ceph
  name: rook-ceph-system
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-csi-cephfs-plugin-sa
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-csi-cephfs-provisioner-sa
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-csi-rbd-plugin-sa
  namespace: ceph-ssd
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: rook-csi-rbd-provisioner-sa
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cephfs-external-provisioner-cfg
  namespace: ceph-ssd
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
  - watch
  - list
  - delete
  - update
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - create
  - delete
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - watch
  - list
  - delete
  - update
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rbd-external-provisioner-cfg
  namespace: ceph-ssd
rules:
- apiGroups:
  - ""
  resources:
  - endpoints
  verbs:
  - get
  - watch
  - list
  - delete
  - update
  - create
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - delete
  - update
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - watch
  - list
  - delete
  - update
  - create
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rook-ceph-cmd-reporter
  namespace: ceph-ssd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rook-ceph-mgr
  namespace: ceph-ssd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - services
  - pods/log
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - ceph.rook.io
  resources:
  - '*'
  verbs:
  - '*'
- apiGroups:
  - apps
  resources:
  - deployments/scale
  - deployments
  verbs:
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rook-ceph-osd
  namespace: ceph-ssd
rules:
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - get
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - ceph.rook.io
  resources:
  - cephclusters
  - cephclusters/finalizers
  verbs:
  - get
  - list
  - create
  - update
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: rook-ceph-purge-osd
  namespace: ceph-ssd
rules:
- apiGroups:
  - ""
  resources:
  - configmaps
  verbs:
  - get
- apiGroups:
  - apps
  resources:
  - deployments
  verbs:
  - get
  - delete
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - get
  - list
  - delete
- apiGroups:
  - ""
  resources:
  - persistentvolumeclaims
  verbs:
  - get
  - update
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/created-by: helm
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/part-of: rook-ceph-operator
    helm.sh/chart: rook-ceph-v1.8.3
    operator: rook
    storage-backend: ceph
  name: rook-ceph-system
  namespace: ceph-ssd
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - configmaps
  - services
  verbs:
  - get
  - list
  - watch
  - patch
  - create
  - update
  - delete
- apiGroups:
  - apps
  - extensions
  resources:
  - daemonsets
  - statefulsets
  - deployments
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - delete
- apiGroups:
  - batch
  resources:
  - cronjobs
  verbs:
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cephfs-csi-provisioner-role-cfg
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cephfs-external-provisioner-cfg
subjects:
- kind: ServiceAccount
  name: rook-csi-cephfs-provisioner-sa
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rbd-csi-provisioner-role-cfg
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rbd-external-provisioner-cfg
subjects:
- kind: ServiceAccount
  name: rook-csi-rbd-provisioner-sa
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-cluster-mgmt
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-cluster-mgmt
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-cmd-reporter
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-cmd-reporter
subjects:
- kind: ServiceAccount
  name: rook-ceph-cmd-reporter
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-mgr
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-mgr
subjects:
- kind: ServiceAccount
  name: rook-ceph-mgr
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-mgr-system
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: rook-ceph-mgr-system
subjects:
- kind: ServiceAccount
  name: rook-ceph-mgr
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-osd
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-osd
subjects:
- kind: ServiceAccount
  name: rook-ceph-osd
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: rook-ceph-purge-osd
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-purge-osd
subjects:
- kind: ServiceAccount
  name: rook-ceph-purge-osd
  namespace: ceph-ssd
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/created-by: helm
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/part-of: rook-ceph-operator
    helm.sh/chart: rook-ceph-v1.8.3
    operator: rook
    storage-backend: ceph
  name: rook-ceph-system
  namespace: ceph-ssd
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: rook-ceph-system
subjects:
- kind: ServiceAccount
  name: rook-ceph-system
  namespace: ceph-ssd
---
apiVersion: v1
data:
  CSI_CEPHFS_FSGROUPPOLICY: None
  CSI_ENABLE_CEPHFS_SNAPSHOTTER: "false"
  CSI_ENABLE_OMAP_GENERATOR: "false"
  CSI_ENABLE_RBD_SNAPSHOTTER: "true"
  CSI_ENABLE_VOLUME_REPLICATION: "false"
  CSI_FORCE_CEPHFS_KERNEL_CLIENT: "true"
  CSI_PLUGIN_ENABLE_SELINUX_HOST_MOUNT: "false"
  CSI_PROVISIONER_REPLICAS: "2"
  CSI_RBD_FSGROUPPOLICY: ReadWriteOnceWithFSType
  CSI_RBD_GRPC_METRICS_PORT: "29090"
  CSI_RBD_LIVENESS_METRICS_PORT: "29080"
  CSI_RBD_PLUGIN_TOLERATIONS: |-
    - key: node.cybozu.io/cluster-not-ready
      operator: Exists
  CSI_RBD_PROVISIONER_TOLERATIONS: |-
    - key: node.cybozu.io/cluster-not-ready
      operator: Exists
  ROOK_CEPH_COMMANDS_TIMEOUT_SECONDS: "300"
  ROOK_CSI_ATTACHER_IMAGE: quay.io/cybozu/csi-attacher:3.4.0.1
  ROOK_CSI_CEPH_IMAGE: quay.io/cybozu/cephcsi:3.5.1.1
  ROOK_CSI_ENABLE_CEPHFS: "false"
  ROOK_CSI_ENABLE_GRPC_METRICS: "false"
  ROOK_CSI_ENABLE_RBD: "true"
  ROOK_CSI_PROVISIONER_IMAGE: quay.io/cybozu/csi-provisioner:3.1.0.1
  ROOK_CSI_REGISTRAR_IMAGE: quay.io/cybozu/csi-node-driver-registrar:2.4.0.1
  ROOK_CSI_RESIZER_IMAGE: quay.io/cybozu/csi-resizer:1.3.0.1
  ROOK_CSI_SNAPSHOTTER_IMAGE: quay.io/cybozu/csi-snapshotter:4.2.0.1
  ROOK_LOG_LEVEL: INFO
  ROOK_OBC_WATCH_OPERATOR_NAMESPACE: "true"
kind: ConfigMap
metadata:
  name: rook-ceph-operator-config
  namespace: ceph-ssd
---
apiVersion: v1
data:
  config: |
    [mgr]
    ; Suppress the folowing warning.
    ;
    ; ```
    ; health: HEALTH_WARN
    ;        1 pools have many more objects per pg than average
    ; ```
    mon_pg_warn_max_object_skew = 0
    [global]
    mon_osd_down_out_subtree_limit = "root"
    ; If HEALTH_WARN appears, it should be investigated, but there is
    ; no requirement for slow ops. So this threshold was extended.
    osd_op_complaint_time = 300.0
kind: ConfigMap
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "-1"
  name: rook-config-override
  namespace: ceph-ssd
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: ceph-extra-exporter
  name: ceph-extra-exporter
  namespace: ceph-ssd
spec:
  ports:
  - name: http
    port: 80
    targetPort: http
  selector:
    app: ceph-extra-exporter
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ceph-extra-exporter
  name: ceph-extra-exporter
  namespace: ceph-ssd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ceph-extra-exporter
  template:
    metadata:
      labels:
        app: ceph-extra-exporter
    spec:
      containers:
      - image: quay.io/cybozu/ceph-extra-exporter:0.1.0
        name: ceph-extra-exporter
        ports:
        - containerPort: 8080
          name: http
        securityContext:
          runAsGroup: 2016
          runAsNonRoot: true
          runAsUser: 2016
        volumeMounts:
        - mountPath: /etc/ceph
          name: ceph-config
      - args:
        - -m
        - -c
        - /usr/local/bin/toolbox.sh
        command:
        - /bin/bash
        env:
        - name: ROOK_CEPH_USERNAME
          valueFrom:
            secretKeyRef:
              key: ceph-username
              name: rook-ceph-mon
        - name: ROOK_CEPH_SECRET
          valueFrom:
            secretKeyRef:
              key: ceph-secret
              name: rook-ceph-mon
        image: quay.io/cybozu/rook:1.8.3.1
        imagePullPolicy: IfNotPresent
        name: toolbox
        securityContext:
          runAsGroup: 2016
          runAsNonRoot: true
          runAsUser: 2016
        tty: true
        volumeMounts:
        - mountPath: /etc/ceph
          name: ceph-config
        - mountPath: /etc/rook
          name: mon-endpoint-volume
      dnsPolicy: ClusterFirstWithHostNet
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 5
      - key: node.cybozu.io/cluster-not-ready
        operator: Exists
      volumes:
      - configMap:
          items:
          - key: data
            path: mon-endpoints
          name: rook-ceph-mon-endpoints
        name: mon-endpoint-volume
      - emptyDir: {}
        name: ceph-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app.kubernetes.io/created-by: helm
    app.kubernetes.io/managed-by: helm
    app.kubernetes.io/part-of: rook-ceph-operator
    helm.sh/chart: rook-ceph-v1.8.3
    operator: rook
    storage-backend: ceph
  name: rook-ceph-operator
  namespace: ceph-ssd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rook-ceph-operator
  template:
    metadata:
      labels:
        app: rook-ceph-operator
        helm.sh/chart: rook-ceph-v1.8.3
    spec:
      containers:
      - args:
        - ceph
        - operator
        env:
        - name: ROOK_CURRENT_NAMESPACE_ONLY
          value: "true"
        - name: ROOK_HOSTPATH_REQUIRES_PRIVILEGED
          value: "false"
        - name: ROOK_ENABLE_SELINUX_RELABELING
          value: "true"
        - name: ROOK_DISABLE_DEVICE_HOTPLUG
          value: "false"
        - name: ROOK_ENABLE_DISCOVERY_DAEMON
          value: "false"
        - name: NODE_NAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        image: quay.io/cybozu/rook:1.8.3.1
        imagePullPolicy: IfNotPresent
        name: rook-ceph-operator
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
        securityContext:
          runAsGroup: 2016
          runAsNonRoot: true
          runAsUser: 2016
        volumeMounts:
        - mountPath: /var/lib/rook
          name: rook-config
        - mountPath: /etc/ceph
          name: default-config-dir
        - mountPath: /etc/webhook
          name: webhook-cert
      serviceAccountName: rook-ceph-system
      volumes:
      - emptyDir: {}
        name: rook-config
      - emptyDir: {}
        name: default-config-dir
      - emptyDir: {}
        name: webhook-cert
---
apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  labels:
    app: rook-ceph-tools
  name: rook-ceph-tools
  namespace: ceph-ssd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rook-ceph-tools
  template:
    metadata:
      labels:
        app: rook-ceph-tools
    spec:
      containers:
      - args:
        - -m
        - -c
        - /usr/local/bin/toolbox.sh
        command:
        - /bin/bash
        env:
        - name: ROOK_CEPH_USERNAME
          valueFrom:
            secretKeyRef:
              key: ceph-username
              name: rook-ceph-mon
        - name: ROOK_CEPH_SECRET
          valueFrom:
            secretKeyRef:
              key: ceph-secret
              name: rook-ceph-mon
        image: quay.io/cybozu/rook:1.8.3.1
        imagePullPolicy: IfNotPresent
        name: rook-ceph-tools
        securityContext:
          runAsGroup: 2016
          runAsNonRoot: true
          runAsUser: 2016
        tty: true
        volumeMounts:
        - mountPath: /etc/ceph
          name: ceph-config
        - mountPath: /etc/rook
          name: mon-endpoint-volume
      dnsPolicy: ClusterFirstWithHostNet
      tolerations:
      - effect: NoExecute
        key: node.kubernetes.io/unreachable
        operator: Exists
        tolerationSeconds: 5
      - key: node.cybozu.io/cluster-not-ready
        operator: Exists
      volumes:
      - configMap:
          items:
          - key: data
            path: mon-endpoints
          name: rook-ceph-mon-endpoints
        name: mon-endpoint-volume
      - emptyDir: {}
        name: ceph-config
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  name: ceph-ssd-block-pool
  namespace: ceph-ssd
spec:
  failureDomain: zone
  replicated:
    size: 3
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
  name: device-health-metrics
  namespace: ceph-ssd
spec:
  failureDomain: zone
  mirroring: {}
  name: device_health_metrics
  parameters:
    compression_mode: none
  replicated:
    requireSafeReplicaSize: true
    size: 3
---
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: ceph-ssd
  namespace: ceph-ssd
spec:
  cephVersion:
    image: quay.io/cybozu/ceph:16.2.7.3
  dashboard:
    ssl: true
  dataDirHostPath: /var/lib/rook
  healthCheck:
    daemonHealth:
      mon:
        timeout: 1800s
    startupProbe:
      osd:
        probe:
          failureThreshold: 60
  mgr:
    count: 2
    modules:
    - enabled: true
      name: pg_autoscaler
  mon:
    count: 3
    volumeClaimTemplate:
      spec:
        resources:
          requests:
            storage: 10Gi
        storageClassName: topolvm-provisioner
  placement:
    all:
      tolerations:
      - key: node.cybozu.io/cluster-not-ready
        operator: Exists
    mgr:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mgr
          topologyKey: topology.kubernetes.io/zone
    mon:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mon
          topologyKey: topology.kubernetes.io/zone
  priorityClassNames:
    crashcollector: rook-node-bound-log
    osd: node-bound
  resources:
    crashcollector:
      requests:
        cpu: 500m
        memory: 60Mi
    mgr:
      requests:
        cpu: 500m
        memory: 512Mi
    mon:
      requests:
        cpu: 500m
        memory: 1Gi
    osd:
      requests:
        cpu: 500m
        memory: 2Gi
    prepareosd:
      requests:
        cpu: 500m
        memory: 50Mi
  storage:
    storageClassDeviceSets:
    - count: 43
      name: set1
      placement:
        topologySpreadConstraints:
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - rook-ceph-osd
              - rook-ceph-osd-prepare
          maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: ScheduleAnyway
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - rook-ceph-osd
              - rook-ceph-osd-prepare
          maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
      volumeClaimTemplates:
      - metadata:
          name: data
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 1Ti
          storageClassName: topolvm-provisioner
          volumeMode: Block
