apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: ceph-ssd
  namespace: ceph-ssd
spec:
  dataDirHostPath: /var/lib/rook
  resources:
    mgr:
      requests:
        cpu: "500m"
        memory: "512Mi"
    mon:
      requests:
        cpu: "500m"
        memory: "1Gi"
    osd:
      requests:
        cpu: "500m"
        memory: "2Gi"
    prepareosd:
      requests:
        cpu: "500m"
        memory: "50Mi"
    crashcollector:
      requests:
        cpu: "500m"
        memory: "60Mi"
    ## please set `requests` parameters on the following modules after enabling it.
    # logcollector:
    #   requests:
    #     cpu: "500m"
    #     memory: "1Gi"
    # cleanup:
    #   requests:
    #     cpu: "500m"
    #     memory: "1Gi"
  mon:
    count: 3
    volumeClaimTemplate:
      spec:
        storageClassName: topolvm-provisioner
        resources:
          requests:
            storage: 10Gi
  mgr:
    count: 2
    modules:
      - name: pg_autoscaler
        enabled: true
  placement:
    mon:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mon
          topologyKey: topology.kubernetes.io/zone
    mgr:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app: rook-ceph-mgr
          topologyKey: topology.kubernetes.io/zone
  cephVersion:
    image: quay.io/cybozu/ceph:16.2.7.0
  dashboard:
    ssl: true
  priorityClassNames:
    osd: node-bound
    crashcollector: rook-node-bound-log
  # disruptionManagement:
  #   managePodBudgets: false
  #   osdMaintenanceTimeout: 60 # rebooting worker nodes takes about 60 minutes.
  #   pgHealthCheckTimeout: 0
  healthCheck:
    daemonHealth:
      # Ensure that mon waits for failover for up to one hour when restarting a node.
      # Rook checks the timeout twice when nodes reboot, set 1800 seconds as the timeout value.
      # NOTE:
      # A bug may cause Rook to perform a timeout check only once during a node reboot.
      # However, this is not a problem since most node reboots are completed within 30 minutes.
      mon:
        timeout: "1800s"
    startupProbe:
      # extend startupProve.failureThreshold for osds since an osd's initialize process is so slow.
      osd:
        probe:
          failureThreshold: 60
  storage:
    storageClassDeviceSets:
      - name: set1
        count: 43
        volumeClaimTemplates:
          - metadata:
              name: data
            spec:
              resources:
                requests:
                  storage: 1Ti
              # IMPORTANT: Change the storage class depending on your environment (e.g. local-storage, gp2)
              storageClassName: topolvm-provisioner
              volumeMode: Block
              accessModes:
                - ReadWriteOnce
        placement:
          topologySpreadConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
                      - rook-ceph-osd-prepare
            - maxSkew: 1
              topologyKey: kubernetes.io/hostname
              whenUnsatisfiable: ScheduleAnyway
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - rook-ceph-osd
                      - rook-ceph-osd-prepare
